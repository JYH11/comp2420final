{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'> COMP2420/COMP6420 - Introduction to Data Management, Analysis and Security</h1>\n",
    "\n",
    "<h2 align='center'> Lab 05 - Data Analysis: Classification </h2>\n",
    "<h5 align='center'><sub> Author: Afzal Ahmad, 2020； Modified by: Cheng Xue and Taylor Qin, 2022. </sub></h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "## Aim\n",
    "Our aim in this lab is:\n",
    "- Understand and implement a logistic regression model for classification\n",
    "- Understand and implement a k-Nearest Neighbour model for classification\n",
    "- Compare the two classification techniques and understand the capabilities and pitfalls of each\n",
    "\n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Outcomes\n",
    "- L03: Demonstrate basic knowledge and understanding of descriptive and predictive data analysis methods, optimization and search, and knowledge representation.\n",
    "- L04: Formulate and extract descriptive and predictive statistics from data\n",
    "- L05: Analyse and interpret results from descriptive and predictive data analysis\n",
    "- L06: Apply their knowledge to a given problem domain and articulate potential data analysis problems\n",
    "\n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Before starting this lab, we suggest you complete the following:\n",
    "- Watch the lectures this week\n",
    "- Complete Lab04 in particular and become familiar with Scikit-Learn's modules\n",
    "\n",
    "\n",
    "The following functions may be useful for this lab:\n",
    "\n",
    "| Function                     | Description |\n",
    "| ---:                         | :---        |\n",
    "| `LogisticRegression()`, `KNeighborsClassifier()` | create an instance of a classification module |\n",
    "| `LabelEncoder()`, `StandardScaler()` | create an instance of a pre-processing module |\n",
    "\n",
    "We have not included functions described in previous labs (especially those used to fit, predict and score models) as we expect you to be familiar with those.\n",
    "\n",
    "*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrew Ha\\AppData\\Local\\Temp\\ipykernel_6656\\3645398992.py:14: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn')\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression     # Logistic Regression\n",
    "from sklearn.neighbors import KNeighborsClassifier      # k-Nearest Neighbours\n",
    "from sklearn.preprocessing import LabelEncoder          # encooding variables\n",
    "from sklearn.preprocessing import StandardScaler        # encooding variables\n",
    "from sklearn.model_selection import train_test_split    # testing our models\n",
    "from sklearn.metrics import confusion_matrix            # scoring\n",
    "\n",
    "import matplotlib.pyplot as plt    # plotting, if you need it\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Not-So-Linear Regression\n",
    "\n",
    "In 1912, the British passenger liner *RMS Titanic* hit an iceberg and sank. Many of the passengers died, and the event is considered to be one of the deadliest marine disasters. Today, we'll be analysing the statistics of the passengers to understand the factors that led to their survivability. We'd like to **predict (or rather, classify) whether a passenger would live or die** depending on factors such as age, gender and passenger class. \n",
    "\n",
    "We will use the data collected from <a href=\"https://www.kaggle.com/c/titanic\">Kaggle</a>. The table below summarises the columns within the data:\n",
    "\n",
    "| Name           | Description |\n",
    "| ---:           | :---        |\n",
    "| `PassengerId`  | an arbitrary ID assigned to each passenger |\n",
    "| `Survived`     | status of passenger's survival<br>(`0`=No, `1`=Yes) |\n",
    "| `Pclass`       | passenger's ticket class<br>(`1`=Upper, `2`=Middle, `3`=Lower) |\n",
    "| `Name`         | full title and name of passenger |\n",
    "| `Sex`          | gender of passenger |\n",
    "| `Age`          | age of passenger<br>fractional if less than 1, xx.5 if estimated |\n",
    "| `SibSp`        | number of siblings and spouses aboard<br>brother / sister / stepbrother / stepsister / husband / wife |\n",
    "| `Parch`        | number of parents and children aboard<br>mother / father / daughter / son / stepdaughter / stepson |\n",
    "| `Ticket`       | ticket ID |\n",
    "| `Fare`         | passenger fare ($) |\n",
    "| `Cabin`        | cabin number |\n",
    "| `Embarked`     | port of embarkation<br>(`C`=Cherbourg, `Q`=Queenstown, `S`=Southampton) |\n",
    "\n",
    "In previous labs, we've given you a lot of guidance on how to deal with data - missing values, choosing your columns, etc. This time we'll give you the freedom (and responsibility) of deciding this for yourself. In making these decisions, feel free to consult classmates, tutors, previous labs and lectures, and online research as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Preparing the Data\n",
    "First, we'll need to **import the data**. The data is located in the file `data/titanic.csv`. Your task is to save it as an object called `titanic` and inspect the first ten rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(url):\n",
    "    \"\"\" \n",
    "    Import data from an address.\n",
    "            Parameters:\n",
    "                    url (string): File path for the data.\n",
    "            Returns:\n",
    "                    data (DataFrame): A dataframe of the data.\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    data = pd.read_csv(url, index_col=0)\n",
    "    return data\n",
    "\n",
    "def first_ten_rows_inspection(data):\n",
    "    \"\"\" \n",
    "    Inspect the first ten rows. \n",
    "            Parameters:\n",
    "                    data (DataFrame): A dataframe of the data.\n",
    "            Returns:\n",
    "                    None.\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    data.head(10)\n",
    "    #raise NotImplementedError\n",
    "    \n",
    "    \n",
    "titanic = import_data(\"data/titanic.csv\")\n",
    "first_ten_rows_inspection(titanic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are your first impressions from this data? You may wish to do some further **data exploration and pre-processing** (for example, finding missing values, the distribution of data, descriptive statistics) to help you understand what you're dealing with.\n",
    "\n",
    "**Note**: Keeping in mind that classifying survival rates is the goal here, process the data to make it useful for a classification model. If you're not sure what to do, this is very similar to prediction (as you did in the last lab), so think of this exercise as a prediction of what you're going to do. \n",
    "\n",
    "Try to consider the following：\n",
    "\n",
    "- Which columns you should drop?\n",
    "- What should you do when you encounter an entry with a missing value? \n",
    "- Do you need to recode any columns?\n",
    "\n",
    "We have written a small scipt to check for the missing values in the data. Feel free to check for other aspects yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived      0\n",
       "Pclass        0\n",
       "Name          0\n",
       "Sex           0\n",
       "Age         177\n",
       "SibSp         0\n",
       "Parch         0\n",
       "Ticket        0\n",
       "Fare          0\n",
       "Cabin       687\n",
       "Embarked      2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 891 entries, 1 to 891\n",
      "Data columns (total 11 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Survived  891 non-null    int64  \n",
      " 1   Pclass    891 non-null    int64  \n",
      " 2   Name      891 non-null    object \n",
      " 3   Sex       891 non-null    object \n",
      " 4   Age       714 non-null    float64\n",
      " 5   SibSp     891 non-null    int64  \n",
      " 6   Parch     891 non-null    int64  \n",
      " 7   Ticket    891 non-null    object \n",
      " 8   Fare      891 non-null    float64\n",
      " 9   Cabin     204 non-null    object \n",
      " 10  Embarked  889 non-null    object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 83.5+ KB\n"
     ]
    }
   ],
   "source": [
    "titanic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_num(sex):\n",
    "    if sex.strip() == 'male':\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yeah! You have successfully preprocessed your data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrew Ha\\AppData\\Local\\Temp\\ipykernel_6656\\1870258032.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Sex'] = data['Sex'].apply(convert_to_num)\n"
     ]
    }
   ],
   "source": [
    "def data_preprocessing(data):\n",
    "    \"\"\" \n",
    "    Prepare your data - drop unneccersary columns, deal with entries with a missing value, etc.\n",
    "            Parameters:\n",
    "                    Original data.\n",
    "            Returns:\n",
    "                    Preprocessed data.\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    data = data[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Fare']]\n",
    "    data['Sex'] = data['Sex'].apply(convert_to_num)\n",
    "    data = data.dropna()\n",
    "    return data\n",
    "    #raise NotImplementedError\n",
    "    \n",
    "titanic = data_preprocessing(titanic)\n",
    "if titanic.isnull().sum().sum() == 0:\n",
    "    print('Yeah! You have successfully preprocessed your data.')\n",
    "else:\n",
    "    print('Not yet! There are some missing values in the data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>714 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass  Sex   Age  SibSp     Fare\n",
       "PassengerId                                             \n",
       "1                   0       3    0  22.0      1   7.2500\n",
       "2                   1       1    1  38.0      1  71.2833\n",
       "3                   1       3    1  26.0      0   7.9250\n",
       "4                   1       1    1  35.0      1  53.1000\n",
       "5                   0       3    0  35.0      0   8.0500\n",
       "...               ...     ...  ...   ...    ...      ...\n",
       "886                 0       3    1  39.0      0  29.1250\n",
       "887                 0       2    0  27.0      0  13.0000\n",
       "888                 1       1    1  19.0      0  30.0000\n",
       "890                 1       1    0  26.0      0  30.0000\n",
       "891                 0       3    0  32.0      0   7.7500\n",
       "\n",
       "[714 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Logistics Regression Implementation\n",
    "We'll be using two classification techniques in this lab. The first is **logistic regression** - which is different from the linear regression in the previous lab. It is a powerful tool especially for binary classification. It's perfect for this exercise, because survivability can take either 0 or 1. Have a look at <a href=\"https://www.youtube.com/watch?v=yIYKR4sgzI8\"> this video</a> if you want to learn more about logistic regression.\n",
    "\n",
    "Have a look at the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\">documentation for Scikit-Learn's Logistic Regression module</a>; you'll need to refer to it for this exercise. Alternatively, you can run `help(LogisticRegression)` to view the documentation through Jupyter (which would be useful for in-lab examinations).\n",
    "\n",
    "You will need to implement the following tasks:\n",
    "\n",
    "1. First **split your data** into training and testing (with 80% training and 20% testing). \n",
    "2. Then **create an instance of the `LogisticRegression()` tool**, \n",
    "3. and **fit the data** using the instance and save it to an object called `logres_model`. When creating the instance, use `solver=lbfgs` and specify `max_iter=1000`. This specifies the method used for optimisation of the model, and allows more iterations for the model to converge.\n",
    "4. After creating the model, use `logres_model.intercept_` and `logres_model.coef_` to **get the coefficients assigned to each column**. You'll need to match the order of the coefficients to the order of the predictors when you fit the model.\n",
    "5. Of course, no machine learning model is useful if you can't make predictions with it. Using the test set that you created earlier, **calculate the train and test scores** of the model (rounding to two decimal places). To increase the score, try adding or removing predictors and compare with classmates to see what they got. Note that the scores here are no longer $R^2$, but **mean accuracy**. We'll explain this in more detail later in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept : [2.31996983]\n",
      "Attributes Coefficients Dictionary:  {'Pclass': -1.1295891552174606, 'Sex': 2.450271366438237, 'Age': -0.03478781492180386, 'SibSp': -0.3773814886948353, 'Fare': 0.0008916027778872573}\n",
      "Train Score: 0.8021015761821366; Test Score: 0.8111888111888111\n"
     ]
    }
   ],
   "source": [
    "def data_split(data):\n",
    "    \"\"\" \n",
    "    Split your data with 80% training and 20% testing.\n",
    "            Parameters:\n",
    "                    Original Data.\n",
    "            Returns:\n",
    "                    Train data;\n",
    "                    Test data.\n",
    "    \"\"\"\n",
    "    train, test = train_test_split(data, test_size = 0.2) # TODO\n",
    "    return train, test\n",
    "\n",
    "def logistic_regression(data):\n",
    "    \"\"\" \n",
    "    Split your data; Create an instance of the LogisticRression() tool; fit the data;\n",
    "            Parameters:\n",
    "                    Original Data.\n",
    "            Returns:\n",
    "                    Logistic Regression Instance;\n",
    "                    Intercept;\n",
    "                    Coef_dict (dict): A dictionary with the keys to be attribute names, \n",
    "                    and the values to be the corresponding coefficients from your model;\n",
    "                    Train_score (rounding to two decimal places);\n",
    "                    Test_score (rounding to two decimal places).\n",
    "    \"\"\"\n",
    "    train, test = data_split(data)\n",
    "    # TODO\n",
    "    train_x = train[['Pclass', 'Sex', 'Age', 'SibSp', 'Fare']]\n",
    "    train_y = train['Survived']\n",
    "    test_x = test[['Pclass', 'Sex', 'Age', 'SibSp', 'Fare']]\n",
    "    test_y = test['Survived']\n",
    "    \n",
    "    lr = LogisticRegression(solver='lbfgs',max_iter=1000)\n",
    "    logres_model = lr.fit(train_x, train_y)\n",
    "    intercept = logres_model.intercept_\n",
    "    coef = {k : co for k, co in zip(train_x.columns, logres_model.coef_[0])}\n",
    "    train_score = logres_model.score(train_x,train_y)\n",
    "    test_score = logres_model.score(test_x, test_y)\n",
    "    #raise NotImplementedError\n",
    "    return logres_model, intercept, coef, train_score, test_score\n",
    "\n",
    "logres_model, intercept, coef, train_score, test_score = logistic_regression(titanic)\n",
    "print(\"Intercept :\", intercept)\n",
    "print(\"Attributes Coefficients Dictionary: \", coef)\n",
    "print(f\"Train Score: {train_score}; Test Score: {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Result Analysis\n",
    "\n",
    "As with prediction, a positive coefficient indicates that a higher predictor leads to a higher probability of the target variable being 1. For example, you might find that the coefficient for `Pclass` is negative - this is because a lower `Pclass` value (eg. First Class) leads to a higher chance of survival. **Unlike linear regression, this doesn't translate directly**; a coefficient of 1.5 does not mean a probability increase of 150%. Instead, it is a **transformation** of the original linear regression formula. If you're interested in learning more, we encourage you to do some online research. As a starting point, try <a href=\"https://machinelearningmastery.com/logistic-regression-for-machine-learning/\">this link</a>. It's likely that you'll study logistic regression in much further detail in future courses at ANU.\n",
    "\n",
    "Please answer the following questions in the text box:\n",
    "1. **Find the coefficient for each predictor and describe its effect** (positive, negative, or insignificant) on survivability. You can (and should) also compare coefficients between predictors (eg. age has a stronger effect than class on survivability).\n",
    "2. Do you think that Logistic Regression is a suitable model for the titanic data? Is it overfitting or underfitting? Why? You should consider looking at the training and test scores."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TODO: Answer here\n",
    "coefficient positive = \n",
    "negative =\n",
    "insignificant = \n",
    "2. looks quite suitable model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 What About Me?\n",
    "Now here's the important question - would you have survived on the Titanic?\n",
    "\n",
    "For each predictor in your model, decide what the value would be for you, pretending that you time travelled to 1912. For `Age` and `Sex` this would be easy, but you'll have to guess what your passenger class would be. If you'd instead prefer to predict the survivability of someone else (or in addition to yourself), consider your favourite TV show, movie or game character.\n",
    "\n",
    "Then, use the `logres_model.predict()` function to find out what your survivability would be. You'll likely run in to errors for this function; ensure that the data you give it is in the correct format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass  Sex   Age  SibSp     Fare\n",
       "PassengerId                                             \n",
       "1                   0       3    0  22.0      1   7.2500\n",
       "2                   1       1    1  38.0      1  71.2833\n",
       "3                   1       3    1  26.0      0   7.9250\n",
       "4                   1       1    1  35.0      1  53.1000\n",
       "5                   0       3    0  35.0      0   8.0500"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You could survive! Yeah :)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrew Ha\\anaconda3\\envs\\comp2420\\lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pred = logres_model.predict([[1,0,22,1,52]]) # TODO: predict survivability using our logres_model for yourself\n",
    "print(\"You could survive! Yeah :)\" if pred==1 else \"It seemed that you couldn't survive :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: In The Neighbourhood\n",
    "The second classification technique we'll learn is **k-Nearest Neighbour**, often shortened to kNN. The general idea (at least, for 1-Nearest Neighbour), is that you make the model memorise all the training data, and when you get a new point for prediction, you match it to the \"most similar\" point in the training set and give it the same label. For kNN, we compare it to the k most similar training points and give it the most common label amongst those points.\n",
    "\n",
    "#### 2.1 Scaling Data, Not Fish\n",
    "Consider two features, `Pclass` and `Age`, and two points:\n",
    "1. `Pclass`=1, `Age`=40, `Survived`=1\n",
    "2. `Pclass`=3, `Age`=20, `Survived`=0\n",
    "\n",
    "You've likely found that `Pclass` is far more important predictor than `Age` - passengers with First Class tickets were more likely to board lifeboats, and thus had a higher chance of surviving. However, `Pclass` has a range of 1-3 while `Age` has a range of 0-80. For k-Nearest Neighbours, this means that comparing to a point like `Pclass`=1, `Age`=20, thus `Survived`=1, the first point above would have a distance of 20 while the second point would have a distance of 2.\n",
    "\n",
    "This is why we need to **scale data**, so that the range of a predictor doesn't affect its distance. To do this, we can use the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\">StandardScaler module in Scikit learn</a>.\n",
    "\n",
    "Because we don't know what the testing data looks like, it would be improper to scale depending on the range of the testing data. So, implement the following:\n",
    "1. Using a StandardScaler instance, **fit and transform only the training data**, naming the transformed data `train_scaled`. \n",
    "2. Then, using the same instance, **transform the testing data separately** (without re-fitting) and name it `test_scaled`.\n",
    "\n",
    "We print the mean and variance of `train_scaled` and `test_scaled` for you. Even you get it worked properly, you might find that, for the training set, they aren't *exactly* 0 and 1, but any difference is insignificant. You should have found a different mean and variance for the scaled testing set; this is because we used the distribution of the training set to scale the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled train data mean:  -2.1154512273067783e-17\n",
      "Scaled train data variance:  0.9999999999999999\n",
      "Scaled test data mean:  0.03159452902937269\n",
      "Scaled test data variance:  1.0536880766120307\n"
     ]
    }
   ],
   "source": [
    "# TODO: scale data\n",
    "def data_scaling(train, test):\n",
    "    \"\"\" \n",
    "    fit and transform the given data.\n",
    "            Parameters:\n",
    "                    Train data;\n",
    "                    Test data.\n",
    "            Returns:\n",
    "                    Scaled train data;\n",
    "                    Scaled test data.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    train_x = train[['Pclass', 'Sex', 'Age', 'SibSp', 'Fare']]\n",
    "    train_y = train['Survived']\n",
    "    test_x = test[['Pclass', 'Sex', 'Age', 'SibSp', 'Fare']]\n",
    "    test_y = test['Survived']\n",
    "    \n",
    "    ss = StandardScaler()\n",
    "    ss_model = ss.fit(train_x)\n",
    "    \n",
    "    train_x_scaled = ss_model.transform(train_x)\n",
    "    test_x_scaled = ss_model.transform(test_x)\n",
    "    \n",
    "    # df = pd.DataFrame(train_scaled)\n",
    "    return train_x_scaled, test_x_scaled\n",
    "    #raise NotImplementedError\n",
    "\n",
    "train, test = data_split(titanic)\n",
    "train_x_scaled, test_x_scaled = data_scaling(train, test)\n",
    "print('Scaled train data mean: ', train_x_scaled.mean())\n",
    "print('Scaled train data variance: ', train_x_scaled.var())\n",
    "print('Scaled test data mean: ', test_x_scaled.mean())\n",
    "print('Scaled test data variance: ', test_x_scaled.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the types of `train_scaled` and `test_scaled` - you'll notice that the scaling module doesn't return a Pandas DataFrame. So that we can apply the same machine learning modules as we have before, convert both of these objects back to Pandas DataFrames, and ensure that their columns are named appropriately. （Hint: renaming columns can be done without explicitly typing out each column name.）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.505391</td>\n",
       "      <td>-0.734223</td>\n",
       "      <td>1.400824</td>\n",
       "      <td>1.566351</td>\n",
       "      <td>1.951751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.895666</td>\n",
       "      <td>-0.734223</td>\n",
       "      <td>0.308881</td>\n",
       "      <td>0.506651</td>\n",
       "      <td>-0.382463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.895666</td>\n",
       "      <td>-0.734223</td>\n",
       "      <td>-0.407706</td>\n",
       "      <td>-0.553048</td>\n",
       "      <td>-0.522825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.895666</td>\n",
       "      <td>-0.734223</td>\n",
       "      <td>1.059592</td>\n",
       "      <td>-0.553048</td>\n",
       "      <td>-0.506758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.304863</td>\n",
       "      <td>-0.734223</td>\n",
       "      <td>-0.919554</td>\n",
       "      <td>-0.553048</td>\n",
       "      <td>-0.155403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>-0.304863</td>\n",
       "      <td>1.361984</td>\n",
       "      <td>-0.032351</td>\n",
       "      <td>0.506651</td>\n",
       "      <td>-0.155403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>0.895666</td>\n",
       "      <td>1.361984</td>\n",
       "      <td>-0.032351</td>\n",
       "      <td>-0.553048</td>\n",
       "      <td>-0.251805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>-1.505391</td>\n",
       "      <td>1.361984</td>\n",
       "      <td>-0.441829</td>\n",
       "      <td>0.506651</td>\n",
       "      <td>1.552928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>-0.304863</td>\n",
       "      <td>1.361984</td>\n",
       "      <td>1.059592</td>\n",
       "      <td>0.506651</td>\n",
       "      <td>-0.150509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>0.895666</td>\n",
       "      <td>-0.734223</td>\n",
       "      <td>0.343005</td>\n",
       "      <td>-0.553048</td>\n",
       "      <td>-0.538321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>571 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pclass       Sex       Age     SibSp      Fare\n",
       "0   -1.505391 -0.734223  1.400824  1.566351  1.951751\n",
       "1    0.895666 -0.734223  0.308881  0.506651 -0.382463\n",
       "2    0.895666 -0.734223 -0.407706 -0.553048 -0.522825\n",
       "3    0.895666 -0.734223  1.059592 -0.553048 -0.506758\n",
       "4   -0.304863 -0.734223 -0.919554 -0.553048 -0.155403\n",
       "..        ...       ...       ...       ...       ...\n",
       "566 -0.304863  1.361984 -0.032351  0.506651 -0.155403\n",
       "567  0.895666  1.361984 -0.032351 -0.553048 -0.251805\n",
       "568 -1.505391  1.361984 -0.441829  0.506651  1.552928\n",
       "569 -0.304863  1.361984  1.059592  0.506651 -0.150509\n",
       "570  0.895666 -0.734223  0.343005 -0.553048 -0.538321\n",
       "\n",
       "[571 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: convert to DataFrames and name columns\n",
    "train_x_scaled = pd.DataFrame(train_x_scaled, columns = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare'])\n",
    "test_x_scaled = pd.DataFrame(test_x_scaled, columns = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare'])\n",
    "train_x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Getting To Know Your Neighbours\n",
    "Look at the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\">documentation for Scikit-Learn's kNN Classifier</a>, or use `help(KNeighborsClassifier)`.\n",
    "\n",
    "Let's continue using the Titanic dataset for predicting survival. Just like you did for logistic regression, \n",
    "1. **Create an instance of the KNeighborsClassifier**. For now, set `n_neighbors=5` (i.e. $k=5$). 、\n",
    "2. Then fit the model and name it `knn_model`. As the instance expects the target variable to have integer values, give it the non-scaled target column for the `y` argument.\n",
    "3. Now find the **training and testing scores** for this model (rounding to two decimal places). Compare this testing score to the testing score you obtained for logistic regression earlier, and also compare your score with other students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.8546409807355516\n",
      "Test score:  0.7762237762237763\n"
     ]
    }
   ],
   "source": [
    "# TODO: fit KNN classifier\n",
    "def knn(data):\n",
    "    \"\"\" \n",
    "    Split and scale your data using what you wrote before; Create an instance of the LogisticRression() tool; fit the data;\n",
    "            Parameters:\n",
    "                    Original Data.\n",
    "            Returns:\n",
    "                    KNN Instance;\n",
    "                    Train_score (rounding to two decimal places);\n",
    "                    Test_score (rounding to two decimal places).\n",
    "    \"\"\"\n",
    "    train,test = data_split(data)\n",
    "    train_x, test_x = data_scaling(train,test)\n",
    "    \n",
    "    train_x_scaled = pd.DataFrame(train_x, columns = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare'])\n",
    "    test_x_scaled = pd.DataFrame(test_x, columns = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare'])\n",
    "    \n",
    "    train_y = train['Survived']\n",
    "    test_y = test['Survived']\n",
    "    \n",
    "    kl = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn_model = kl.fit(train_x_scaled, train_y)\n",
    "    \n",
    "    train_score = knn_model.score(train_x_scaled, train_y)\n",
    "    test_score = knn_model.score(test_x_scaled, test_y)\n",
    "    #raise NotImplementedError\n",
    "    return knn_model, train_score, test_score\n",
    "\n",
    "knn_model, train_score_knn, test_score_knn = knn(titanic) \n",
    "print(\"Training Score:\", train_score_knn)\n",
    "print(\"Test score: \", test_score_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 How Big Should Our Neighbourhood Be?\n",
    "Earlier, we used `n_neighbors=5` when creating the kNN instance. Try increasing or decreasing this parameter and see how it affects the model performance. Note that `k` is a hyperparameter of knn, so to avoid overfitting to the test set, we need:\n",
    "1. firstly **create a validation set**. \n",
    "2. **Find the best `k` on the validation set and evaluate the model with the best `k` on the test set**. You can either adjust the code you wrote previously, or copy it here and adjust it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "    trains,test = data_split(titanic)\n",
    "    train_x, test_x = data_scaling(train,test)\n",
    "    \n",
    "    train_x_scaled = pd.DataFrame(train_x, columns = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare'])\n",
    "    test_x_scaled = pd.DataFrame(test_x, columns = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare'])\n",
    "    \n",
    "    train_y = train['Survived']\n",
    "    test_y = test['Survived']\n",
    "    \n",
    "    train_x, val_x, train_y, val_y = train_test_split(train_x_scaled, train_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for n_neigh = 1 train_score = 0.9824561403508771 val_score = 0.808695652173913\n",
      "for n_neigh = 2 train_score = 0.8881578947368421 val_score = 0.7739130434782608\n",
      "for n_neigh = 3 train_score = 0.8706140350877193 val_score = 0.8260869565217391\n",
      "for n_neigh = 4 train_score = 0.8399122807017544 val_score = 0.7913043478260869\n",
      "for n_neigh = 5 train_score = 0.8442982456140351 val_score = 0.7913043478260869\n",
      "for n_neigh = 6 train_score = 0.8333333333333334 val_score = 0.7478260869565218\n",
      "for n_neigh = 7 train_score = 0.8333333333333334 val_score = 0.782608695652174\n",
      "for n_neigh = 8 train_score = 0.831140350877193 val_score = 0.7565217391304347\n",
      "for n_neigh = 10 train_score = 0.8289473684210527 val_score = 0.7565217391304347\n",
      "for n_neigh = 20 train_score = 0.8179824561403509 val_score = 0.808695652173913\n",
      "for n_neigh = 30 train_score = 0.7982456140350878 val_score = 0.7913043478260869\n",
      "for n_neigh = 40 train_score = 0.7828947368421053 val_score = 0.7739130434782608\n",
      "for n_neigh = 456 train_score = 0.6118421052631579 val_score = 0.5652173913043478\n"
     ]
    }
   ],
   "source": [
    "for i in [1,2,3,4,5,6,7,8,10,20,30,40, train_x.shape[0]]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn_model = knn.fit(train_x, train_y)\n",
    "    \n",
    "    train_score = knn_model.score(train_x, train_y)\n",
    "    val_score = knn_model.score(val_x, val_y)\n",
    "    \n",
    "    print(f'for n_neigh = {i} train_score = {train_score} val_score = {val_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the best choice of parameter value? Try NOT to fine-tune it too much (as this can lead to overfitting in your model, and you shouldn't be using the testing score to adjust your model). \n",
    "\n",
    "What would happen when we set `n_neighbors=N`, where `N` is the number of entries in the training set? Alternatively, what about `n_neighbors=1`? (Hint: think about what a small difference in the predictors values of a new point would cause.)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TODO: understand the n_neighbors parameter\n",
    "n=1 keep changing the value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Classifying Flowers\n",
    "It's likely that you found a higher testing score, and a much more accurate training score, for logistic regression than for kNN. However, as we've mentioned before, (binary) logistic regression has a major pitfall: it can only classify two-class variables.\n",
    "\n",
    "Of course, we can use an advanced form of logistic regression, called Multinomial Logistic Regression (not to be confused with Multiple Linear Regression), but the theory for that technique is beyond the scope of this course. Instead, we'll simply use **kNN** here for multi-class classification.\n",
    "\n",
    "Let's go back to the Iris dataset. Your tasks are as follows:\n",
    "\n",
    "1. **Import and explore the data** (`data/IRIS.csv`) so that you're familiar with it (if you're not already).\n",
    "2. As you did with the Titanic dataset, **transform the data** as necessary,\n",
    "3. **split the data** into training and testing (80-20, ensuring that each set is representative of the whole dataset), \n",
    "4. **scale the data** according to the training set distribution, \n",
    "5. **fit a new model** using a new instance of the kNN classifier, and finally **find the training and testing scores** of the model with this data.\n",
    "\n",
    "You could feel free to use the previously-written helper functions for the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_num(species):\n",
    "    if species.strip() == 'Iris-setosa':\n",
    "        return 0\n",
    "    elif species.strip() == 'Iris-virginica':\n",
    "        return 1\n",
    "    return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yeah! You have successfully preprocessed your data.\n"
     ]
    }
   ],
   "source": [
    "def data_preprocessing(data):\n",
    "    \"\"\" \n",
    "    Prepare your data - drop unneccersary columns, deal with entries with a missing value, etc.\n",
    "            Parameters:\n",
    "                    Original data.\n",
    "            Returns:\n",
    "                    Preprocessed data.\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    data = data[['sepal_length','sepal_width', 'petal_length', 'petal_width', 'species']]\n",
    "    data['species'] = data['species'].apply(convert_to_num)\n",
    "    data = data.dropna()\n",
    "    return data\n",
    "    #raise NotImplementedError\n",
    "    \n",
    "titanic = data_preprocessing(data)\n",
    "if titanic.isnull().sum().sum() == 0:\n",
    "    print('Yeah! You have successfully preprocessed your data.')\n",
    "else:\n",
    "    print('Not yet! There are some missing values in the data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled train data mean:  -3.293661639721298e-16\n",
      "Scaled train data variance:  1.0000000000000002\n",
      "Scaled test data mean:  -0.055440940607275536\n",
      "Scaled test data variance:  1.1119771079625986\n"
     ]
    }
   ],
   "source": [
    "def data_scaling(train, test):\n",
    "    \"\"\" \n",
    "    fit and transform the given data.\n",
    "            Parameters:\n",
    "                    Train data;\n",
    "                    Test data.\n",
    "            Returns:\n",
    "                    Scaled train data;\n",
    "                    Scaled test data.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    train_x = train[['sepal_length','sepal_width', 'petal_length', 'petal_width']]\n",
    "    train_y = train['species']\n",
    "    test_x = test[['sepal_length','sepal_width', 'petal_length', 'petal_width']]\n",
    "    test_y = test['species']\n",
    "    \n",
    "    ss = StandardScaler()\n",
    "    ss_model = ss.fit(train_x)\n",
    "    \n",
    "    train_x_scaled = ss_model.transform(train_x)\n",
    "    test_x_scaled = ss_model.transform(test_x)\n",
    "    \n",
    "    # df = pd.DataFrame(train_scaled)\n",
    "    return train_x_scaled, test_x_scaled\n",
    "    #raise NotImplementedError\n",
    "\n",
    "train, test = data_split(titanic)\n",
    "train_x_scaled, test_x_scaled = data_scaling(train, test)\n",
    "print('Scaled train data mean: ', train_x_scaled.mean())\n",
    "print('Scaled train data variance: ', train_x_scaled.var())\n",
    "print('Scaled test data mean: ', test_x_scaled.mean())\n",
    "print('Scaled test data variance: ', test_x_scaled.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fit KNN classifier\n",
    "def knn(data):\n",
    "    \"\"\" \n",
    "    Split and scale your data using what you wrote before; Create an instance of the LogisticRression() tool; fit the data;\n",
    "            Parameters:\n",
    "                    Original Data.\n",
    "            Returns:\n",
    "                    KNN Instance;\n",
    "                    Train_score (rounding to two decimal places);\n",
    "                    Test_score (rounding to two decimal places).\n",
    "    \"\"\"\n",
    "    train,test = data_split(data)\n",
    "    train_x, test_x = data_scaling(train,test)\n",
    "    \n",
    "    train_x_scaled = pd.DataFrame(train_x, columns = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare'])\n",
    "    test_x_scaled = pd.DataFrame(test_x, columns = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare'])\n",
    "    \n",
    "    train_y = train['Survived']\n",
    "    test_y = test['Survived']\n",
    "    \n",
    "    kl = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn_model = kl.fit(train_x_scaled, train_y)\n",
    "    \n",
    "    train_score = knn_model.score(train_x_scaled, train_y)\n",
    "    test_score = knn_model.score(test_x_scaled, test_y)\n",
    "    #raise NotImplementedError\n",
    "    return knn_model, train_score, test_score\n",
    "\n",
    "knn_model, train_score_knn, test_score_knn = knn(titanic) \n",
    "print(\"Training Score:\", train_score_knn)\n",
    "print(\"Test score: \", test_score_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9583333333333334\n",
      "Test score:  1.0\n"
     ]
    }
   ],
   "source": [
    "# TODO: your code here\n",
    "data = pd.read_csv('data/IRIS.csv')\n",
    "\n",
    "data = data[['sepal_length','sepal_width', 'petal_length', 'petal_width', 'species']]\n",
    "data['species'] = data['species'].apply(convert_to_num)\n",
    "data = data.dropna()\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "\n",
    "train_x = train[['sepal_length','sepal_width', 'petal_length', 'petal_width']]\n",
    "train_y = train['species']\n",
    "test_x = test[['sepal_length','sepal_width', 'petal_length', 'petal_width']]\n",
    "test_y = test['species']\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss_model = ss.fit(train_x)\n",
    "train_x_scaled = ss_model.transform(train_x)\n",
    "test_x_scaled = ss_model.transform(test_x)\n",
    "\n",
    "train_x_scaled = pd.DataFrame(train_x_scaled, columns = ['sepal_length','sepal_width', 'petal_length', 'petal_width'])\n",
    "test_x_scaled = pd.DataFrame(test_x_scaled, columns = ['sepal_length','sepal_width', 'petal_length', 'petal_width'])\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model = knn.fit(train_x_scaled, train_y)\n",
    "    \n",
    "train_score = knn_model.score(train_x_scaled, train_y)\n",
    "test_score = knn_model.score(test_x_scaled, test_y)\n",
    "print(\"Training Score:\", train_score)\n",
    "print(\"Test score: \", test_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did everything right, you should be getting fairly high scores. Run the code a few times using different, random train-test partitions to get a better understanding of the average score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Confusing You Some More\n",
    "We've explored the default mean accuracy score (using `model.score()`), but classification also has other important scoring techniques that are useful for diagnosing your model. To start off, let's **create a confusion matrix** using <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\">Scikit-Learn's Confusion Matrix module</a>. Using the testing data for the Titanic dataset and the logistic regression model you created, produce a confusion matrix. Ensure that you give the function the right parameter values (use `help(confusion_matrix)` if needed). (Hint: you'll need to use the `model.predict()` function to create `y_pred`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a pretty confusing (pun intended) set of numbers there. What do they mean?\n",
    "\n",
    "The confusion matrix is made up of $n$ columns and $n$ rows, where $n$ is the number of target levels you have (2 for the Titanic dataset). The rows indicate the observations, or actuals, while the columns indicate the predicted, starting from the lowest level. Specifically, if $C$ is the confusion matrix, then $C_{0,0}$ is the **true negatives** (predicted negative, actually negative), $C_{0,1}$ is the **false positives** (predicted positive, actually negative), $C_{1,0}$ is the **false negatives** (predicted negative, actually positive) and $C_{1,1}$ is the **true positives** (predicted positive, actually positive). As we want correct predictions, we want $C_{0,0}$ and $C_{1,1}$ (i.e. values on the main diagonal) to be as large as possible. The documentation for this module also explains this.\n",
    "\n",
    "Re-produce the confusion matrix, but this time save it to four new objects by using `tn, fp, fn, tp = ...` and using the `ravel()` function (the `confusion_matrix()` documentation has an example of this). This will keep a record of each of the four numbers described above. We've provided a (crude) way of show the confusion matrix counts, their labels, counts and sums; if you've completed the previous steps correctly you should be able to just run this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 PREDICTION\n",
      "                __0_____1__\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6656\\2177585045.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"                 PREDICTION\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"                __0_____1__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"OBSERVATION  0 |\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrjust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"  \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrjust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"|\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtn\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"             1 |\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrjust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"  \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrjust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"|\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"               ------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tn' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: save confusion matrix counts\n",
    "\n",
    "print(\"                 PREDICTION\")\n",
    "print(\"                __0_____1__\")\n",
    "print(\"OBSERVATION  0 |\", str(tn).rjust(2), \"  \", str(fp).rjust(2), \"|\", tn+fp)\n",
    "print(\"             1 |\", str(fn).rjust(2), \"  \", str(tp).rjust(2), \"|\", fn+tp)\n",
    "print(\"               ------------\")\n",
    "print(\"                \", tn+fn, \"  \", fp+tp, \" \", tn+fp+fn+tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can compare the predictions and observations. If your model is behaving unexpectedly, you can use the confusion matrix to easily determine whether the model is only predicting one label. Confusion matrices are also important if you especially want to avoid a particular type of incorrect prediction. For example, a cancer screening that incorrectly classifies a person as not having cancer when they do have cancer is life-threatening, so you'd want to alter your model to avoid that.\n",
    "\n",
    "Now let's calculate a few different scoring metrics:\n",
    "- **Recall**: TP / (TP + FN). This describes the proportion of actual-positive observations that were correctly classified.\n",
    "- **Precision**: TP / (TP + FP). This is the percentage of positive-predicted observations that were correctly classified.\n",
    "- **Accuracy**: (TP + TN) / (TP+FP+FN+TN). This is the percentage of correctly classified observations in total. This is the same as the `model.score()` function that we used earlier.\n",
    "- **F1**: (2 * Recall * Prediction) / (Recall + Prediction). This is a weighted average of recall and precision, and generally a better metric than accuracy for data that is unbalanced with respect to its target labels.\n",
    "\n",
    "**Find each of the scores above** for your Titanic logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: calculate recall, precision, accuracy and F1 scores\n",
    "recall = None # TODO\n",
    "prec = None # TODO\n",
    "acc = None # TODO\n",
    "f1 = None # TODO\n",
    "print(\"Recall:   \", recall,\n",
    "    \"\\nPrecision:\", prec,\n",
    "    \"\\nAccuracy: \", acc,\n",
    "    \"\\nF1 Score: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look through these scores and understand what they mean for your model. Are these scores fairly similar? If not, how come?\n",
    "\n",
    "Re-fit the kNN model for the Titanic dataset (especially if you've played around with the `n_neighbors` parameter), and repeat the above steps to **calculate the four metrics for the kNN model**. Compare the two models' metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: repeat for kNN model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens to the confusion matrix if you change the `n_neighbors` parameter to be equal to the size of the training data? Fit the kNN model with `n_neighbors=N`, where `N` is the size of the training set, and view the output of the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change n_neighbors and look at confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the output for the confusion matrix matches with your answer to the previous question when you were adjusting the `n_neighbors` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework & Extension Questions\n",
    "You will need to complete previous exercises before starting these exercises.\n",
    "\n",
    "### Exercise 4: Scaled or Un-Scaled?\n",
    "In an earlier exercise, we showed that scaling was necessary for the kNN classifier. Now, for both the Titanic and Iris datasets, **fit new models using un-scaled data** and compare the predictive scores. What do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fit un-scaled kNN model for Titanic and compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fit un-scaled kNN model for Iris and compare\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one of these, you'll find a noticeable improvement in the performance, while the other might be more or less the same as when you used scaled data. Investigate the datasets and **figure out why scaling has a larger impact on one model**. *Hint: look at the descriptive statistics for both datasets. Which statistic(s) are most relevant?*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO: investigate why scaling depends on dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Looking In The Grey Area\n",
    "Logistic regression has another advantage that we haven't mentioned: it can produce a \"reliable\" *probability* of success, rather than a black-and-white \"success or fail\" output. While the kNN classifier can also do this (by comparing the targets of its nearest neighbours), this isn't as reliable and it depends heavily on the `n_neighbors` parameter.\n",
    "\n",
    "Have a look at the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\">documentation for the Logistic Regression module</a> again (or use `help(LogisticRegression)`) and find out which function can be used to calculate the probability of success.\n",
    "\n",
    "Then, repeat the prediction for yourself and/or a character, and **report the probability of survival**. If you had to guess some of the predictor values for that person, try altering them slightly and see how it affects the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: find probability of survival for yourself and/or a character\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
